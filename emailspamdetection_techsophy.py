# -*- coding: utf-8 -*-
"""EmailSpamDetection-TechSophy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NGGWEkB2URLQx2SJsix8y19gWM5sXlrv
"""

from google.colab import files
uploaded = files.upload()

"""# Importing necessary libraries"""

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import nltk

from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from xgboost import XGBClassifier

"""# Reading dataset"""

df = pd.read_csv('email.csv')

"""# Data Exploration"""

df.head()

df.tail()

df.shape

df.columns

df.duplicated().sum()

df.info()

df.describe()

df.nunique()

nltk.download('stopwords')

df.drop_duplicates(inplace=True)
df.dropna(inplace=True)

df.Category.value_counts()

[df.Category == '{"mode":"full"']

if 5572 in df.index:
    df.drop(index=5572, inplace=True)

print("Dataset Shape:", df.shape)
print("Category Distribution:\n", df['Category'].value_counts())

"""# Data Visualization

Let's visualize the distribution of 'ham' and 'spam' emails in the dataset to understand the class imbalance.
"""

plt.figure(figsize=(6, 4))
sns.countplot(x='Category', data=df)
plt.title('Distribution of Ham vs. Spam Emails')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(x='Category', data=df)
plt.title('Distribution of Ham vs. Spam Emails')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

df['message_length'] = df['Message'].apply(len)

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='message_length', hue='Category', kde=True, bins=50)
plt.title('Distribution of Message Lengths by Category')
plt.xlabel('Message Length')
plt.ylabel('Frequency')
plt.show()

from wordcloud import WordCloud
import re
import string
import nltk
from nltk.corpus import stopwords


try:
    nltk.data.find('corpora/stopwords')
except nltk.downloader.DownloadError:
    nltk.download('stopwords')


def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)
    text = re.sub(r"@\w+|#", '', text)
    text = re.sub(r"[^a-zA-Z\s]", '', text)
    text = text.strip()
    return text

stop_words = set(stopwords.words('english'))
def remove_stopwords(text):
    return ' '.join([word for word in text.split() if word not in stop_words])

if 'clean_email' not in df.columns:
    df['clean_email'] = df['Message'].apply(clean_text).apply(remove_stopwords)


ham_messages = df[df['Category'] == 'ham']['clean_email']
spam_messages = df[df['Category'] == 'spam']['clean_email']

all_ham_messages = " ".join(ham_messages)
all_spam_messages = " ".join(spam_messages)

plt.figure(figsize=(10, 7))
ham_wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(all_ham_messages)
plt.imshow(ham_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in Ham Emails')
plt.show()

plt.figure(figsize=(10, 7))
spam_wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(all_spam_messages)
plt.imshow(spam_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in Spam Emails')
plt.show()

"""**Reasoning**:
Calculate and visualize the top frequent words for both ham and spam categories using bar plots.


"""

from collections import Counter

def get_top_n_words(text, n=None):
    words = text.split()
    word_counts = Counter(words)
    return word_counts.most_common(n)

top_ham_words = get_top_n_words(all_ham_messages, 30)
top_spam_words = get_top_n_words(all_spam_messages, 30)

plt.figure(figsize=(12, 6))
ham_words_df = pd.DataFrame(top_ham_words, columns=['Word', 'Count'])
sns.barplot(x='Count', y='Word', data=ham_words_df, palette='viridis')
plt.title('Top 30 Most Frequent Words in Ham Emails')
plt.xlabel('Frequency')
plt.ylabel('Word')
plt.show()

plt.figure(figsize=(12, 6))
spam_words_df = pd.DataFrame(top_spam_words, columns=['Word', 'Count'])
sns.barplot(x='Count', y='Word', data=spam_words_df, palette='plasma')
plt.title('Top 30 Most Frequent Words in Spam Emails')
plt.xlabel('Frequency')
plt.ylabel('Word')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='message_length', hue='Category', kde=True, bins=50)
plt.title('Distribution of Message Lengths by Category')
plt.xlabel('Message Length')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(x='Category', data=df)
plt.title('Distribution of Ham vs. Spam Emails')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

df['message_length'] = df['Message'].apply(len)

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='message_length', hue='Category', kde=True, bins=50)
plt.title('Distribution of Message Lengths by Category')
plt.xlabel('Message Length')
plt.ylabel('Frequency')
plt.show()



"""# Label Encoding and Cleaning"""

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)
    text = re.sub(r"@\w+|#", '', text)
    text = re.sub(r"[^a-zA-Z\s]", '', text)
    text = text.strip()
    return text

stop_words = set(stopwords.words('english'))
def remove_stopwords(text):
    return ' '.join([word for word in text.split() if word not in stop_words])

df['clean_email'] = df['Message'].apply(clean_text).apply(remove_stopwords)

label_encoder = LabelEncoder()
df['label_encoded'] = label_encoder.fit_transform(df['Category'])

"""# Using TF-IDF"""

vectorizer = TfidfVectorizer(max_features=3000)
X = vectorizer.fit_transform(df['clean_email']).toarray()
y = df['label_encoded']

print("TF-IDF Feature Shape:", X.shape)

"""# Training the Data"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

from sklearn.metrics import accuracy_score, f1_score
from sklearn.linear_model import LogisticRegression, RidgeClassifier, PassiveAggressiveClassifier, SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

models = {
    "Logistic Regression": LogisticRegression(),
    "Ridge Classifier": RidgeClassifier(),
    "Passive Aggressive": PassiveAggressiveClassifier(),
    "SGD Classifier": SGDClassifier(),
    "Naive Bayes": MultinomialNB(),
    "Random Forest": RandomForestClassifier(),
    "Extra Trees": ExtraTreesClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "LightGBM": LGBMClassifier()
}

import time
from sklearn.metrics import precision_score, recall_score

results = []

for name, model in models.items():
    print(f"Training: {name}")

    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time

    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)

    results.append({
        "Model": name,
        "Accuracy": round(accuracy, 4),
        "F1 Score": round(f1, 4),
        "Precision": round(precision, 4),
        "Recall": round(recall, 4),
        "Train Time (s)": round(train_time, 2)
    })
results_df = pd.DataFrame(results).sort_values(by='Accuracy', ascending=False)

results_df.style.background_gradient(cmap='YlGnBu').format(precision=4)

plt.figure(figsize=(12, 6))
sns.barplot(data=results_df.melt(id_vars="Model", value_vars=["Accuracy", "F1 Score", "Precision", "Recall"]),
            x="Model", y="value", hue="variable")
plt.xticks(rotation=45, ha='right')
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.tight_layout()
plt.show()

def predict_email_category(email_message):

    cleaned_email = clean_text(email_message)
    cleaned_email = remove_stopwords(cleaned_email)

    email_vector = vectorizer.transform([cleaned_email])

    extra_trees_model = models["Extra Trees"]
    prediction = extra_trees_model.predict(email_vector)

    predicted_category = label_encoder.inverse_transform(prediction)[0]

    return predicted_category

"""Now you can test the prediction function with some example emails.

# Evaluating the Results
"""

example_email_1 = "Congratulations! You've won a free trip!"
prediction_1 = predict_email_category(example_email_1)
print(f"Email: '{example_email_1}'\nPredicted Category: {prediction_1}\n")

example_email_2 = "Hey, just wanted to remind you about our meeting tomorrow."
prediction_2 = predict_email_category(example_email_2)
print(f"Email: '{example_email_2}'\nPredicted Category: {prediction_2}\n")

example_email_3 = "URGENT! Your account has been compromised. Click here to verify."
prediction_3 = predict_email_category(example_email_3)
print(f"Email: '{example_email_3}'\nPredicted Category: {prediction_3}\n")

example_email_4 = "Free money! Claim your prize now by clicking this link."
prediction_4 = predict_email_category(example_email_4)
print(f"Email: '{example_email_4}'\nPredicted Category: {prediction_4}")

"""# Task
We have an unbalanced dataset of spam and ham, and now we need to balance it using techniques like SMOTE

## Identify and confirm imbalance

### Subtask:
Explicitly show the class distribution to confirm the imbalance.

**Reasoning**:
Calculate and print the class distribution of the original dataframe and the training set to explicitly show the imbalance.
"""

print("Original DataFrame Category Distribution:\n", df['Category'].value_counts())
print("\nTraining Set Category Distribution (before SMOTE):\n", y_train.value_counts())

"""## Apply smote

### Subtask:
Apply SMOTE to the training data (X_train, y_train) to balance the classes.

**Reasoning**:
Apply SMOTE to balance the training data and confirm the class distribution.
"""

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print("Training Set Category Distribution (after SMOTE):\n", y_train_resampled.value_counts())

"""## Retrain models

### Subtask:
Retrain the models using the balanced training data (X_train_resampled, y_train_resampled).

**Reasoning**:
Iterate through the models and retrain them using the SMOTE-resampled training data.
"""

resampled_results = []

for name, model in models.items():
    print(f"Training with Resampled Data: {name}")

    start_time = time.time()
    model.fit(X_train_resampled, y_train_resampled)
    train_time = time.time() - start_time

    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)

    resampled_results.append({
        "Model": name,
        "Accuracy": round(accuracy, 4),
        "F1 Score": round(f1, 4),
        "Precision": round(precision, 4),
        "Recall": round(recall, 4),
        "Train Time (s)": round(train_time, 2)
    })

resampled_results_df = pd.DataFrame(resampled_results).sort_values(by='Accuracy', ascending=False)

resampled_results_df.style.background_gradient(cmap='YlGnBu').format(precision=4)

print("Performance Results Before SMOTE:")
display(results_df.style.background_gradient(cmap='YlGnBu').format(precision=4))

print("\nPerformance Results After SMOTE:")
display(resampled_results_df.style.background_gradient(cmap='YlGnBu').format(precision=4))

print("\nAnalysis of Results:")
print("Comparing the performance metrics before and after applying SMOTE:")
print("- Accuracy: Some models like Extra Trees and Random Forest show a slight decrease in accuracy after SMOTE, while others like Naive Bayes and Passive Aggressive show a slight increase.")
print("- F1 Score: Many models show an improvement in F1 Score after SMOTE (e.g., Logistic Regression, Ridge Classifier, Passive Aggressive, SGD Classifier, Naive Bayes, SVM, KNN), indicating a better balance between precision and recall for the minority class.")
print("- Precision: Precision generally decreases for most models after SMOTE. This is expected as the model becomes less conservative in predicting the minority class (spam) due to the increased samples, leading to more false positives.")
print("- Recall: Recall significantly increases for most models after SMOTE (e.g., Logistic Regression, Ridge Classifier, Passive Aggressive, SGD Classifier, Naive Bayes, KNN). This is a key benefit of using SMOTE, as it helps the model identify more of the actual minority class instances (spam emails).")
print("\nOverall, SMOTE has helped to improve the recall of the minority class (spam) for many models, which is crucial for spam detection. While precision might decrease for some models, the increase in recall and often F1 score indicates a better ability to detect spam emails that were previously missed due to the class imbalance.")

"""### Visualize word frequencies

**Reasoning**:
Generate and display word clouds for both ham and spam messages to visually represent the most frequent words in each category.
"""

from wordcloud import WordCloud

ham_messages = df[df['Category'] == 'ham']['clean_email']
spam_messages = df[df['Category'] == 'spam']['clean_email']

all_ham_messages = " ".join(ham_messages)
all_spam_messages = " ".join(spam_messages)

plt.figure(figsize=(10, 7))
ham_wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(all_ham_messages)
plt.imshow(ham_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in Ham Emails')
plt.show()

plt.figure(figsize=(10, 7))
spam_wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(all_spam_messages)
plt.imshow(spam_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Words in Spam Emails')
plt.show()

"""**Reasoning**:
Calculate and visualize the top frequent words for both ham and spam categories using bar plots.
"""

from collections import Counter

def get_top_n_words(text, n=None):
    words = text.split()
    word_counts = Counter(words)
    return word_counts.most_common(n)

top_ham_words = get_top_n_words(all_ham_messages, 30)
top_spam_words = get_top_n_words(all_spam_messages, 30)

plt.figure(figsize=(12, 6))
ham_words_df = pd.DataFrame(top_ham_words, columns=['Word', 'Count'])
sns.barplot(x='Count', y='Word', data=ham_words_df, palette='viridis')
plt.title('Top 30 Most Frequent Words in Ham Emails')
plt.xlabel('Frequency')
plt.ylabel('Word')
plt.show()

plt.figure(figsize=(12, 6))
spam_words_df = pd.DataFrame(top_spam_words, columns=['Word', 'Count'])
sns.barplot(x='Count', y='Word', data=spam_words_df, palette='plasma')
plt.title('Top 30 Most Frequent Words in Spam Emails')
plt.xlabel('Frequency')
plt.ylabel('Word')
plt.show()

"""### Visualize data distribution after SMOTE

**Reasoning**:
Visualize the distribution of 'ham' and 'spam' in the training data after applying SMOTE to show the balanced classes.
"""

plt.figure(figsize=(6, 4))
sns.countplot(x=y_train_resampled)
plt.title('Distribution of Ham vs. Spam Emails (Training Data After SMOTE)')
plt.xlabel('Category (0: Ham, 1: Spam)')
plt.ylabel('Count')
plt.xticks([0, 1], ['Ham', 'Spam'])
plt.show()

"""### Implement priority assignment

**Reasoning**:
Write a function to assign priority levels based on the predicted category and presence of urgent keywords.
"""

def assign_priority(email_message, predicted_category):
    if predicted_category == 'spam':
        return 'Low'
    else:
        urgent_keywords = ['urgent', 'important', 'meeting', 'action required', 'time sensitive']
        if any(keyword in email_message.lower() for keyword in urgent_keywords):
            return 'High'
        else:
            return 'Medium'

"""### Display Results

**Reasoning**:
Use the example emails to predict their category and assign a priority level, then display the results.
"""

example_emails = [
    "Congratulations! You've won a free trip!",
    "Hey, just wanted to remind you about our meeting tomorrow.",
    "URGENT! Your account has been compromised. Click here to verify.",
    "Free money! Claim your prize now by clicking this link.",
    "This is an important message regarding your account.",
    "Just a quick reminder about the upcoming deadline."
]

for email in example_emails:
    predicted_category = predict_email_category(email)
    priority = assign_priority(email, predicted_category)
    print(f"Email: '{email}'")
    print(f"Predicted Category: {predicted_category}")
    print(f"Assigned Priority: {priority}\n")

